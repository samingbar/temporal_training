{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139ac51d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Temporal activities for BERT checkpointing, fine-tuning, and inference.\n",
    "\n",
    "This module contains the *non-deterministic* parts of the BERT checkpointing\n",
    "example:\n",
    "\n",
    "- Creating content-addressed dataset snapshots for reproducible experiments.\n",
    "- Long-running, compute-heavy fine-tuning with mid-run checkpoints.\n",
    "- Loading a saved checkpoint and running batch inference.\n",
    "\n",
    "The corresponding Temporal workflows orchestrate these activities, but all of\n",
    "the actual ML logic (dataset loading, tokenization, model forward passes, etc.)\n",
    "stays here so that workflow code can remain deterministic and replay-safe.\n",
    "\"\"\"\n",
    "import os\n",
    "import datetime\n",
    "import asyncio\n",
    "import contextlib\n",
    "import hashlib\n",
    "import json\n",
    "import queue\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Final\n",
    "from httpcore import request\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, set_seed, TrainerCallback\n",
    "\n",
    "try:\n",
    "    from datasets import ClassLabel\n",
    "except Exception:\n",
    "    ClassLabel = None\n",
    "\n",
    "from temporalio import activity\n",
    "from temporalio.client import Client\n",
    "from temporalio.contrib.pydantic import pydantic_data_converter\n",
    "\n",
    "from src.workflows.train_tune.bert_parallel.custom_types import (\n",
    "    BertFineTuneRequest,\n",
    "    BertFineTuneResult,\n",
    "    BertInferenceRequest,\n",
    "    BertInferenceResult,\n",
    "    CheckpointInfo,\n",
    "    DatasetSnapshotRequest,\n",
    "    DatasetSnapshotResult,\n",
    "    BertEvalRequest,\n",
    "    BertEvalResult,\n",
    ")\n",
    "\n",
    "try:  # Heavy ML imports are only needed when activities actually run.\n",
    "    import torch\n",
    "    from datasets import load_dataset\n",
    "    from transformers import (\n",
    "        AutoModelForSequenceClassification,\n",
    "        AutoTokenizer,\n",
    "        Trainer,\n",
    "        TrainingArguments,\n",
    "    )\n",
    "except Exception:  # noqa: BLE001\n",
    "    # The unit tests patch out the sync helpers, so importing ML deps is optional\n",
    "    # for test runs. At runtime, the helpers below will raise a clear error if\n",
    "    # the libraries are missing.\n",
    "    torch = None  # type: ignore[assignment]\n",
    "    load_dataset = None  # type: ignore[assignment]\n",
    "    AutoModelForSequenceClassification = None  # type: ignore[assignment]\n",
    "    AutoTokenizer = None  # type: ignore[assignment]\n",
    "    Trainer = None  # type: ignore[assignment]\n",
    "    TrainingArguments = None  # type: ignore[assignment]\n",
    "\n",
    "\n",
    "# Human-friendly error message surfaced when ML dependencies are missing. This keeps\n",
    "# the Temporal worker process healthy even if the Python environment is not configured\n",
    "# for running the BERT example.\n",
    "TRANSFORMERS_IMPORT_MESSAGE: Final[str] = (\n",
    "    \"BERT checkpointing dependencies are not installed. \"\n",
    "    \"Install 'transformers', 'datasets', and 'torch' to execute this activity.\"\n",
    ")\n",
    "\n",
    "# How frequently the fine-tuning activity should send heartbeats while training is\n",
    "# running in a background thread. This example uses a modest interval suitable for\n",
    "# both local development and the 5s per-test timeout configured in pytest.\n",
    "HEARTBEAT_INTERVAL_SECONDS: Final[float] = 5.0\n",
    "\n",
    "\n",
    "class BertFineTuneActivities:\n",
    "    \"\"\"Activity collection for checkpoint-aware BERT fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.config = None\n",
    "        self.tokenizer = None\n",
    "        self.text_field: str | None = None\n",
    "        self.text_pair_field: str | None = None\n",
    "        self.label_field: str | None = None\n",
    "        self.task_type: str | None = None  # \"classification\" | \"regression\"\n",
    "        self.num_labels: int | None = None\n",
    "    \n",
    "    def _infer_text_fields(self, sample: dict) -> None:\n",
    "        \"\"\"Infer (text_field, text_pair_field) from config overrides or dataset columns.\"\"\"\n",
    "        # 1) Config overrides win.\n",
    "        if getattr(self.config, \"text_field\", None):\n",
    "            self.text_field = self.config.text_field\n",
    "            self.text_pair_field = getattr(self.config, \"text_pair_field\", None)\n",
    "            return\n",
    "\n",
    "        COMMON_TEXT_COLS = (\"text\", \"sentence\", \"content\", \"review\", \"question\", \"article\", \"prompt\")\n",
    "        COMMON_PAIR_COLS = (\n",
    "            (\"sentence1\", \"sentence2\"),\n",
    "            (\"premise\", \"hypothesis\"),\n",
    "            (\"question\", \"context\"),\n",
    "            (\"query\", \"passage\"),\n",
    "        )\n",
    "\n",
    "        # 2) Common pair schemas.\n",
    "        for a, b in COMMON_PAIR_COLS:\n",
    "            if a in sample and b in sample and isinstance(sample[a], str) and isinstance(sample[b], str):\n",
    "                self.text_field, self.text_pair_field = a, b\n",
    "                return\n",
    "\n",
    "        # 3) Common single text field names.\n",
    "        for c in COMMON_TEXT_COLS:\n",
    "            if c in sample and isinstance(sample[c], str):\n",
    "                self.text_field = c\n",
    "                self.text_pair_field = None\n",
    "                return\n",
    "\n",
    "        # 4) Fallback: first string field.\n",
    "        for k, v in sample.items():\n",
    "            if isinstance(v, str):\n",
    "                self.text_field = k\n",
    "                self.text_pair_field = None\n",
    "                return\n",
    "\n",
    "        raise KeyError(f\"Couldn't infer a text column from dataset columns: {list(sample.keys())}\")\n",
    "\n",
    "    def _infer_label_field_and_task(self, train_features, sample: dict) -> None:\n",
    "        \"\"\"Infer label column, task type, and num_labels (or use config overrides).\"\"\"\n",
    "        # 1) Config override for label field\n",
    "        self.label_field = getattr(self.config, \"label_field\", None)\n",
    "\n",
    "        # If not provided, try common names.\n",
    "        if self.label_field is None:\n",
    "            for c in (\"label\", \"labels\", \"target\", \"score\", \"y\"):\n",
    "                if c in sample:\n",
    "                    self.label_field = c\n",
    "                    break\n",
    "\n",
    "        if self.label_field is None:\n",
    "            # Last-resort fallback: try any numeric scalar column\n",
    "            for k, v in sample.items():\n",
    "                if isinstance(v, (int, float)) and k not in (self.text_field, self.text_pair_field):\n",
    "                    self.label_field = k\n",
    "                    break\n",
    "\n",
    "        if self.label_field is None:\n",
    "            raise KeyError(f\"Couldn't infer a label column from dataset columns: {list(sample.keys())}\")\n",
    "\n",
    "        # 2) Infer task type (or honor config override)\n",
    "        cfg_task = getattr(self.config, \"task_type\", \"auto\")\n",
    "        feature = train_features.get(self.label_field)\n",
    "\n",
    "        if cfg_task in (\"classification\", \"regression\"):\n",
    "            self.task_type = cfg_task\n",
    "        else:\n",
    "            # Auto mode: if ClassLabel -> classification, if float -> regression, else classification.\n",
    "            if ClassLabel is not None and isinstance(feature, ClassLabel):\n",
    "                self.task_type = \"classification\"\n",
    "            else:\n",
    "                v = sample[self.label_field]\n",
    "                self.task_type = \"regression\" if isinstance(v, float) else \"classification\"\n",
    "\n",
    "        # 3) Infer num_labels\n",
    "        if self.task_type == \"regression\":\n",
    "            self.num_labels = 1\n",
    "            return\n",
    "\n",
    "        # classification\n",
    "        if ClassLabel is not None and isinstance(feature, ClassLabel):\n",
    "            self.num_labels = int(feature.num_classes)\n",
    "        else:\n",
    "            # simple heuristic: gather a small set of unique labels from the first ~1k examples\n",
    "            # (keeps it simple; avoids scanning the whole dataset)\n",
    "            self.num_labels = None  # caller can fill using dataset slice if desired\n",
    "    \n",
    "\n",
    "    def compute_metrics(self, eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "\n",
    "        logits = np.asarray(logits)\n",
    "        labels = np.asarray(labels)\n",
    "\n",
    "        # Normalize label shape to (N,)\n",
    "        if labels.ndim > 1:\n",
    "            labels = labels.reshape(-1)\n",
    "\n",
    "        # Regression: common HF convention is num_labels == 1\n",
    "        if getattr(self.config, \"num_labels\", None) == 1 or logits.ndim == 1 or logits.shape[-1] == 1:\n",
    "            preds = logits.reshape(-1)\n",
    "            mse = float(np.mean((preds - labels) ** 2))\n",
    "            rmse = float(np.sqrt(mse))\n",
    "            return {\"mse\": mse, \"rmse\": rmse}\n",
    "\n",
    "        # Classification\n",
    "        preds = np.argmax(logits, axis=-1)\n",
    "        acc = float(np.mean(preds == labels))\n",
    "\n",
    "        metrics = {\"accuracy\": acc}\n",
    "\n",
    "        # Add a simple binary F1 when it looks binary\n",
    "        if logits.shape[-1] == 2:\n",
    "            tp = float(np.sum((preds == 1) & (labels == 1)))\n",
    "            fp = float(np.sum((preds == 1) & (labels == 0)))\n",
    "            fn = float(np.sum((preds == 0) & (labels == 1)))\n",
    "\n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "            f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "            metrics.update({\"precision\": precision, \"recall\": recall, \"f1\": f1})\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def tokenize_function(self, batch: dict) -> dict:\n",
    "        if self.tokenizer is None or self.config is None:\n",
    "            raise RuntimeError(\"Tokenizer/config not initialized; call _fine_tune_bert_sync first.\")\n",
    "\n",
    "        # Single-text or text-pair tokenization depending on what we inferred.\n",
    "        # Be defensive in case we're mapping over a split whose schema differs\n",
    "        # from the one we originally inspected.\n",
    "        text_field = self.text_field\n",
    "        text_pair_field = self.text_pair_field\n",
    "\n",
    "        if text_field not in batch or (text_pair_field is not None and text_pair_field not in batch):\n",
    "            # Re-infer from the current batch's first example.\n",
    "            sample: dict = {}\n",
    "            for k, v in batch.items():\n",
    "                # `v` is typically a list/array of values for this column.\n",
    "                if isinstance(v, (list, tuple)) and v:\n",
    "                    sample[k] = v[0]\n",
    "                else:\n",
    "                    sample[k] = v\n",
    "\n",
    "            self._infer_text_fields(sample)\n",
    "            text_field = self.text_field\n",
    "            text_pair_field = self.text_pair_field\n",
    "\n",
    "        if text_pair_field is not None:\n",
    "            return self.tokenizer(\n",
    "                batch[text_field],\n",
    "                batch[text_pair_field],\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=self.config.max_seq_length,\n",
    "            )\n",
    "\n",
    "        return self.tokenizer(\n",
    "            batch[text_field],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.config.max_seq_length,\n",
    "        )\n",
    "    def _cast_labels(self,batch: dict) -> dict:\n",
    "        ys = batch[\"labels\"]\n",
    "        if self.task_type == \"regression\":\n",
    "            return {\"labels\": [float(y) for y in ys]}\n",
    "        return {\"labels\": [int(y) for y in ys]}\n",
    "\n",
    "    def _fine_tune_bert_sync(\n",
    "        self,\n",
    "        request: BertFineTuneRequest,\n",
    "        checkpoint_queue: \"queue.Queue[CheckpointInfo] | None\" = None,\n",
    "    ) -> BertFineTuneResult:\n",
    "        \"\"\"Run a BERT fine-tuning job with optional checkpointing.\n",
    "\n",
    "        This helper encapsulates *all* ML details and knows nothing about Temporal.\n",
    "        The async activity wrapper offloads to this helper in a thread so that:\n",
    "\n",
    "        - The code can be imported and unit-tested without a Temporal worker.\n",
    "        - The Temporal worker can keep polling for new tasks while training runs.\n",
    "        \"\"\"\n",
    "        if torch is None or load_dataset is None:\n",
    "            # pragma: no cover - only hit when deps are actually missing\n",
    "            raise RuntimeError(TRANSFORMERS_IMPORT_MESSAGE)\n",
    "        \n",
    "        start_time = time.perf_counter()\n",
    "        self.config = request.config\n",
    "\n",
    "        if self.config.seed is not None:\n",
    "            set_seed(self.config.seed)\n",
    "        else:\n",
    "            set_seed(42)\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # 2. Load the dataset, preferring a pre-materialized snapshot when\n",
    "        #    provided for full reproducibility.\n",
    "        # ------------------------------------------------------------------\n",
    "        if request.dataset_snapshot is not None:\n",
    "            snapshot_path = Path(request.dataset_snapshot.snapshot_path)\n",
    "            data_path = snapshot_path / \"data.jsonl\"\n",
    "            raw_datasets = load_dataset(\"json\", data_files=str(data_path))\n",
    "        else:\n",
    "            raw_datasets = load_dataset(self.config.dataset_name, self.config.dataset_config_name)\n",
    "        # ------------------------------------------------------------------\n",
    "        schema_split_name = \"train\" if \"train\" in raw_datasets else next(iter(raw_datasets.keys()))\n",
    "        schema_split = raw_datasets[schema_split_name]\n",
    "\n",
    "        schema_sample = schema_split[0]  # just inspect columns/types\n",
    "\n",
    "\n",
    "        # Infer the primary text field for this dataset (e.g. \"sentence\" for GLUE,\n",
    "        # \"text\" for IMDB) so tokenization works across multiple sources.\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_name)\n",
    "        \n",
    "        # Infer text fields (or honor config overrides)\n",
    "        self._infer_text_fields(schema_sample)\n",
    "\n",
    "        # Infer label field + task type (+ maybe num_labels)\n",
    "        self._infer_label_field_and_task(schema_split.features, schema_sample)\n",
    "\n",
    "        # If classification and we couldn't get num_labels from features, estimate cheaply\n",
    "        if self.task_type == \"classification\" and self.num_labels is None:\n",
    "            probe_n = min(1000, len(schema_split))\n",
    "            label_probe = schema_split.select(range(probe_n))[self.label_field]\n",
    "            self.num_labels = int(len(set(label_probe)))\n",
    "\n",
    "        # Apply the tokenizer across the dataset; `batched=True` lets HF process\n",
    "        # multiple rows at once for better throughput.\n",
    "        tokenized_datasets = raw_datasets.map(self.tokenize_function, batched=True)\n",
    "\n",
    "        # Normalize label column to \"labels\" for Trainer\n",
    "        if self.label_field != \"labels\":\n",
    "            tokenized_datasets = tokenized_datasets.rename_column(self.label_field, \"labels\")\n",
    "\n",
    "            \n",
    "        tokenized_datasets = tokenized_datasets.map(self._cast_labels, batched=True)\n",
    "        # Tell Datasets to yield PyTorch tensors for the columns the Trainer needs.\n",
    "        tokenized_datasets.set_format(\n",
    "            type=\"torch\",\n",
    "            columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "        )\n",
    " \n",
    "        # ------------------------------------------------------------------\n",
    "        # 3. Prepare train and eval datasets, applying any sub-sampling requested.\n",
    "        eval_dataset = (\n",
    "            tokenized_datasets.get(\"validation\")\n",
    "            or tokenized_datasets.get(\"validation_matched\")\n",
    "            or tokenized_datasets.get(\"dev\")\n",
    "            or tokenized_datasets.get(\"val\")\n",
    "            )\n",
    "        \n",
    "        if eval_dataset is None and \"train\" in tokenized_datasets:\n",
    "            split = tokenized_datasets[\"train\"].train_test_split(\n",
    "                test_size=0.1,\n",
    "                seed=self.config.seed,\n",
    "            )\n",
    "            tokenized_datasets[\"train\"] = split[\"train\"]\n",
    "            eval_dataset = split[\"test\"]\n",
    "\n",
    "        train_dataset = tokenized_datasets[\"train\"]\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # 3. Optionally sub-sample train/eval for a fast demo run on laptops.\n",
    "        if self.config.shuffle_before_select:\n",
    "            train_dataset = train_dataset.shuffle(seed=self.config.seed)\n",
    "            if eval_dataset is not None:\n",
    "                eval_dataset = eval_dataset.shuffle(seed=self.config.seed)\n",
    "\n",
    "        if self.config.max_train_samples is not None and self.config.max_train_samples < len(train_dataset):\n",
    "            train_dataset = train_dataset.select(range(self.config.max_train_samples))\n",
    "\n",
    "        if eval_dataset is not None and self.config.max_eval_samples is not None and self.config.max_eval_samples < len(eval_dataset):\n",
    "            eval_dataset = eval_dataset.select(range(self.config.max_eval_samples))\n",
    "\n",
    "        # 4. Construct the classification head on top of the base encoder.\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            self.config.model_name,\n",
    "            num_labels=int(self.num_labels or 2),\n",
    "        )\n",
    "\n",
    "        if self.task_type == \"regression\":\n",
    "                model.config.problem_type = \"regression\"\n",
    "        else:\n",
    "                model.config.problem_type = \"single_label_classification\"\n",
    "\n",
    "        # Make regression explicit\n",
    "        if self.task_type == \"regression\":\n",
    "            model.config.problem_type = \"regression\"\n",
    "\n",
    "        # 5. Configure the Transformers Trainer with step-based checkpointing.\n",
    "        #    We keep the configuration simple and tuned for readability over\n",
    "        #    state-of-the-art results. If there's no eval dataset (e.g. when\n",
    "        #    training purely on a JSONL snapshot with only a train split),\n",
    "        #    disable evaluation to avoid Trainer complaining.\n",
    "        steps_per_epoch = max(1, len(train_dataset) // self.config.batch_size)\n",
    "        # Aim for a couple of checkpoints per epoch when possible.\n",
    "        save_steps = max(1, steps_per_epoch // 2)\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"./bert_runs/{request.run_id}\",\n",
    "            num_train_epochs=float(self.config.num_epochs),\n",
    "            per_device_train_batch_size=self.config.batch_size,\n",
    "            per_device_eval_batch_size=self.config.batch_size,\n",
    "            learning_rate=self.config.learning_rate,\n",
    "            eval_strategy=\"epoch\" if eval_dataset is not None else \"no\",\n",
    "            save_strategy=\"steps\",\n",
    "            save_steps=save_steps,\n",
    "            save_total_limit=3,\n",
    "            logging_strategy=\"steps\",\n",
    "            logging_steps=save_steps,\n",
    "            report_to=[],\n",
    "            load_best_model_at_end=False,\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            compute_metrics=self.compute_metrics if eval_dataset is not None else None,\n",
    "        )\n",
    "\n",
    "        # If we're resuming after a worker restart or a prior run, prefer an\n",
    "        # explicitly provided checkpoint path, otherwise detect the latest\n",
    "        # checkpoint in the output directory (if any) so we don't start from 0.\n",
    "        output_dir = Path(training_args.output_dir)\n",
    "        resume_path = request.resume_from_checkpoint\n",
    "        if resume_path is None and output_dir.exists():\n",
    "            existing_checkpoints = sorted(p for p in output_dir.glob(\"checkpoint-*\") if p.is_dir())\n",
    "            if existing_checkpoints:\n",
    "                resume_path = str(existing_checkpoints[-1])\n",
    "\n",
    "\n",
    "        # Attach a callback that enqueues checkpoint metadata on each save, if requested.\n",
    "        if checkpoint_queue is not None:\n",
    "            trainer.add_callback(\n",
    "                QueueingCheckpointCallback(\n",
    "                    checkpoint_queue=checkpoint_queue,\n",
    "                    num_epochs=self.config.num_epochs,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        # Run training, but fall back to a fresh start if the checkpoint is\n",
    "        # incompatible with the current model configuration (e.g., model_name\n",
    "        # or num_labels changed between runs).\n",
    "        try:\n",
    "            train_result = trainer.train(resume_from_checkpoint=resume_path)\n",
    "        except RuntimeError as exc:  # pragma: no cover - defensive path\n",
    "            msg = str(exc)\n",
    "            if \"Error(s) in loading state_dict\" in msg and \"size mismatch for\" in msg:\n",
    "                activity.logger.warning(\n",
    "                    \"Incompatible checkpoint detected for run %s (likely model_name/label schema changed); \"\n",
    "                    \"restarting training from scratch without resuming.\",\n",
    "                    request.run_id,\n",
    "                )\n",
    "                train_result = trainer.train()\n",
    "            else:\n",
    "                raise\n",
    "        \n",
    "        eval_metrics: dict[str, float] | None = None\n",
    "\n",
    "        if eval_dataset is not None:\n",
    "            raw_metrics = trainer.evaluate()\n",
    "\n",
    "            # Keep only numeric scalars and normalize to plain floats\n",
    "            eval_metrics = {\n",
    "                k: float(v)\n",
    "                for k, v in raw_metrics.items()\n",
    "                if isinstance(v, (int, float))\n",
    "            }\n",
    "\n",
    "        # Persist the final fine-tuned model and tokenizer so that the inference\n",
    "        # activity can load them later based solely on ``run_id``.\n",
    "        trainer.save_model()\n",
    "        self.tokenizer.save_pretrained(training_args.output_dir)\n",
    "\n",
    "        # Discover any mid-run checkpoints created by the Trainer.\n",
    "        checkpoint_dirs = sorted(p for p in output_dir.glob(\"checkpoint-*\") if p.is_dir())\n",
    "        total_checkpoints_saved = len(checkpoint_dirs)\n",
    "\n",
    "        num_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        training_time_seconds = float(time.perf_counter() - start_time)\n",
    "\n",
    "        return BertFineTuneResult(\n",
    "            run_id=request.run_id,\n",
    "            config=self.config,\n",
    "            train_loss=float(train_result.training_loss),\n",
    "            eval_accuracy=eval_metrics,\n",
    "            training_time_seconds=training_time_seconds,\n",
    "            num_parameters=num_parameters,\n",
    "            dataset_snapshot=request.dataset_snapshot,\n",
    "            total_checkpoints_saved=total_checkpoints_saved,\n",
    "            inferred_text_field=self.text_field,\n",
    "            inferred_text_pair_field=self.text_pair_field,\n",
    "            inferred_label_field=self.label_field,\n",
    "            inferred_task_type=self.task_type,\n",
    "            inferred_num_labels=self.num_labels,\n",
    "        )\n",
    "\n",
    "    @activity.defn\n",
    "    async def fine_tune_bert(self, request: BertFineTuneRequest) -> BertFineTuneResult:\n",
    "        \"\"\"Temporal activity that runs a BERT fine-tuning job.\"\"\"\n",
    "        activity.logger.info(\n",
    "            \"Starting BERT fine-tuning run %s with model %s on %s/%s\",\n",
    "            request.run_id,\n",
    "            request.config.model_name,\n",
    "            request.config.dataset_name,\n",
    "            request.config.dataset_config_name,\n",
    "        )\n",
    "\n",
    "        # Offload the training to a separate thread and send periodic heartbeats\n",
    "        # so Temporal can detect liveness during long-running fine-tuning.\n",
    "        #\n",
    "        # This pattern lets us:\n",
    "        # - Keep the heavy ML work off the event loop thread, and\n",
    "        # - Give Temporal visibility into progress via heartbeats, which in turn\n",
    "        #   enables heartbeat timeouts and cancellation handling.\n",
    "\n",
    "        # Shared queue for checkpoint updates produced by the Trainer callback.\n",
    "        checkpoint_queue: \"queue.Queue[CheckpointInfo]\" = queue.Queue()\n",
    "\n",
    "        # Best-effort initialization of a workflow handle used for signaling.\n",
    "        signal_handle = None\n",
    "        try:\n",
    "            info = activity.info()\n",
    "            client = await Client.connect(\n",
    "                \"localhost:7233\",\n",
    "                data_converter=pydantic_data_converter,\n",
    "            )\n",
    "            signal_handle = client.get_workflow_handle(\n",
    "                info.workflow_id,\n",
    "                run_id=info.workflow_run_id,\n",
    "            )\n",
    "        except Exception:  # noqa: BLE001\n",
    "            activity.logger.exception(\n",
    "                \"Failed to initialize checkpoint signaling; continuing without it\",\n",
    "            )\n",
    "\n",
    "        training_task = asyncio.create_task(\n",
    "            asyncio.to_thread(self._fine_tune_bert_sync, request, checkpoint_queue),\n",
    "        )\n",
    "        try:\n",
    "            while not training_task.done():\n",
    "                activity.heartbeat({\"run_id\": request.run_id})\n",
    "\n",
    "                # Drain any newly produced checkpoints and signal them to the workflow.\n",
    "                if signal_handle is not None:\n",
    "                    while True:\n",
    "                        try:\n",
    "                            checkpoint_info = checkpoint_queue.get_nowait()\n",
    "                        except queue.Empty:\n",
    "                            break\n",
    "                        try:\n",
    "                            await signal_handle.signal(\"update_checkpoint\", checkpoint_info)\n",
    "                        except Exception:  # noqa: BLE001\n",
    "                            activity.logger.exception(\n",
    "                                \"Failed to signal checkpoint %s\",\n",
    "                                checkpoint_info.path,\n",
    "                            )\n",
    "\n",
    "                await asyncio.sleep(HEARTBEAT_INTERVAL_SECONDS)\n",
    "            result = await training_task\n",
    "        except asyncio.CancelledError:  # pragma: no cover - cancellation path\n",
    "            training_task.cancel()\n",
    "            with contextlib.suppress(Exception):\n",
    "                await training_task\n",
    "            raise\n",
    "\n",
    "        # After training completes, flush any remaining checkpoints in the queue.\n",
    "        if signal_handle is not None:\n",
    "            try:\n",
    "                while True:\n",
    "                    checkpoint_info = checkpoint_queue.get_nowait()\n",
    "                    try:\n",
    "                        await signal_handle.signal(\"update_checkpoint\", checkpoint_info)\n",
    "                    except Exception:  # noqa: BLE001\n",
    "                        activity.logger.exception(\n",
    "                            \"Failed to signal checkpoint %s\",\n",
    "                            checkpoint_info.path,\n",
    "                        )\n",
    "            except queue.Empty:\n",
    "                pass\n",
    "\n",
    "        # Log a concise summary of training metrics, handling both scalar and dict shapes.\n",
    "        if result.eval_accuracy is None:\n",
    "            eval_summary = \"N/A\"\n",
    "        elif isinstance(result.eval_accuracy, dict):\n",
    "            if \"accuracy\" in result.eval_accuracy:\n",
    "                eval_summary = f\"accuracy={float(result.eval_accuracy['accuracy']):.3f}\"\n",
    "            else:\n",
    "                eval_summary = \", \".join(\n",
    "                    f\"{k}={float(v):.3f}\"\n",
    "                    for k, v in result.eval_accuracy.items()\n",
    "                    if isinstance(v, (int, float))\n",
    "                )\n",
    "        else:\n",
    "            eval_summary = f\"{float(result.eval_accuracy):.3f}\"\n",
    "\n",
    "        activity.logger.info(\n",
    "            \"Completed BERT fine-tuning run %s with loss %.4f and metrics %s\",\n",
    "            request.run_id,\n",
    "            result.train_loss,\n",
    "            eval_summary,\n",
    "        )\n",
    "        return result\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
