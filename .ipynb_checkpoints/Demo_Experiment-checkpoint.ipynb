{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18a9a4e6-0e34-45cd-a12a-75f571231d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== BERT evaluation summary ===\n",
      "run_id                               dataset              split        examples  accuracy\n",
      "-----------------------------------------------------------------------------------------\n",
      "Bert-ladder-sweep-5765ab1d-d3e8-4484-9a81-8e4dc1aaf2aa-s0-0003-ladder-total glue/sst2            validation        872     0.920\n",
      "Bert-ladder-sweep-5765ab1d-d3e8-4484-9a81-8e4dc1aaf2aa-baseline-01226826-510a-4ed7-9bdb-c7efe9176d3c glue/sst2            validation        872     0.805\n",
      "\n",
      "Best run Bert-ladder-sweep-5765ab1d-d3e8-4484-9a81-8e4dc1aaf2aa-s0-0003-ladder-total improved accuracy by 0.115 over the baseline comparison run (baseline=0.805, best=0.920).\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import random\n",
    "import uuid\n",
    "\n",
    "from temporalio.client import Client\n",
    "from temporalio.contrib.pydantic import pydantic_data_converter\n",
    "\n",
    "from src.workflows.train_tune.bert_sweeps.custom_types import (\n",
    "    BertEvalRequest,\n",
    "    BertFineTuneConfig,\n",
    "    CoordinatorWorkflowConfig,\n",
    "    SweepRequest,\n",
    "    SweepSpace,\n",
    ")\n",
    "from src.workflows.train_tune.bert_sweeps.workflows import (\n",
    "    LadderSweepWorkflow,\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Sample Sweep Configurations\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "# Sample 1: {Model: BERT Uncased - Dataset: Glue}\n",
    "config_1 = CoordinatorWorkflowConfig(\n",
    "    fine_tune_config=BertFineTuneConfig(\n",
    "        model_name=\"bert-base-uncased\",\n",
    "        dataset_name=\"glue\",\n",
    "        dataset_config_name=\"sst2\",\n",
    "        num_epochs=2,\n",
    "        batch_size=8,\n",
    "        learning_rate=2e-5,\n",
    "        max_seq_length=128,\n",
    "        use_gpu=True,\n",
    "        max_train_samples=3_000,\n",
    "        max_eval_samples=2_000,\n",
    "        seed=random.randint(0, 10000),\n",
    "    ),\n",
    "    evaluation_config=BertEvalRequest(\n",
    "        dataset_name=\"glue\",\n",
    "        dataset_config_name=\"sst2\",\n",
    "        split=\"validation\",\n",
    "        max_eval_samples=1_000,\n",
    "        max_seq_length=128,\n",
    "        batch_size=32,\n",
    "        use_gpu=True,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Sample 2 {Model: BERT Cased - Dataset: Glue}\n",
    "config_2 = CoordinatorWorkflowConfig(\n",
    "    fine_tune_config=BertFineTuneConfig(\n",
    "        model_name=\"bert-base-cased\",\n",
    "        dataset_name=\"glue\",\n",
    "        dataset_config_name=\"sst2\",\n",
    "        num_epochs=10,\n",
    "        batch_size=16,\n",
    "        learning_rate=3e-5,\n",
    "        max_seq_length=128,\n",
    "        use_gpu=True,\n",
    "        max_train_samples=3_000,\n",
    "        max_eval_samples=2_000,\n",
    "        seed=random.randint(0, 10000),\n",
    "    ),\n",
    "    evaluation_config=BertEvalRequest(\n",
    "        dataset_name=\"glue\",\n",
    "        dataset_config_name=\"sst2\",\n",
    "        split=\"validation\",\n",
    "        max_eval_samples=1_000,\n",
    "        max_seq_length=128,\n",
    "        batch_size=32,\n",
    "        use_gpu=True,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Sample 3 {Model: BERT Uncased - Dataset: IMDB}\n",
    "seed = random.randint(0, 10000)\n",
    "config_3 = CoordinatorWorkflowConfig(\n",
    "    fine_tune_config=BertFineTuneConfig(\n",
    "        model_name=\"bert-base-uncased\",\n",
    "        dataset_name=\"imdb\",\n",
    "        dataset_config_name=\"plain_text\",\n",
    "        num_epochs=10,\n",
    "        batch_size=32,\n",
    "        learning_rate=2e-5,\n",
    "        max_seq_length=256,\n",
    "        use_gpu=True,\n",
    "        max_train_samples=5_000,\n",
    "        max_eval_samples=2_000,\n",
    "        seed=seed,\n",
    "    ),\n",
    "    evaluation_config=BertEvalRequest(\n",
    "        dataset_name=\"imdb\",\n",
    "        dataset_config_name=\"plain_text\",\n",
    "        split=\"test\",\n",
    "        max_eval_samples=1_000,\n",
    "        max_seq_length=256,\n",
    "        batch_size=32,\n",
    "        use_gpu=True,\n",
    "        seed=seed,\n",
    "    ),\n",
    ")\n",
    "# Sample 4: {Model: DistilBERT - Dataset: Glue}\n",
    "seed = random.randint(0, 10000)\n",
    "config_4 = CoordinatorWorkflowConfig(\n",
    "    fine_tune_config=BertFineTuneConfig(\n",
    "        model_name=\"distilbert-base-uncased\",\n",
    "        dataset_name=\"glue\",\n",
    "        dataset_config_name=\"sst2\",\n",
    "        num_epochs=2,\n",
    "        batch_size=4,  # MPS-safe\n",
    "        learning_rate=5e-5,\n",
    "        max_seq_length=64,  # MPS-safe\n",
    "        use_gpu=True,\n",
    "        max_train_samples=2_000,\n",
    "        max_eval_samples=1_000,\n",
    "        shuffle_before_select=True,\n",
    "        seed=seed,\n",
    "        # optional overrides if you added them:\n",
    "        # text_field=None,\n",
    "        # text_pair_field=None,\n",
    "        # label_field=None,\n",
    "        # task_type=\"auto\",\n",
    "    ),\n",
    "    evaluation_config=BertEvalRequest(\n",
    "        # run_id will be filled in by the coordinator after training\n",
    "        run_id=None,\n",
    "        dataset_name=\"glue\",\n",
    "        dataset_config_name=\"sst2\",\n",
    "        split=\"validation\",\n",
    "        max_eval_samples=1_000,\n",
    "        max_seq_length=64,\n",
    "        batch_size=32,\n",
    "        use_gpu=True,\n",
    "        seed=seed,\n",
    "        # if you changed eval to require model_uri/model_path, leave it unset here\n",
    "        # and let the coordinator populate it from the training result.\n",
    "        # model_path=None,\n",
    "        # model_uri=None,\n",
    "    ),\n",
    "    dataset_snapshot=None,  # or pass a DatasetSnapshotResult if you want reproducibility\n",
    ")\n",
    "\n",
    "# Sample 5: {Model: MiniLM-L12-H384-uncased - Dataset: Glue}\n",
    "seed = random.randint(0, 10000)\n",
    "config_5 = CoordinatorWorkflowConfig(\n",
    "    fine_tune_config=BertFineTuneConfig(\n",
    "        model_name=\"microsoft/MiniLM-L12-H384-uncased\",\n",
    "        dataset_name=\"glue\",\n",
    "        dataset_config_name=\"sst2\",\n",
    "        num_epochs=2,\n",
    "        batch_size=4,  # still MPS-safe\n",
    "        learning_rate=3e-5,  # MiniLM often prefers slightly lower LR\n",
    "        max_seq_length=64,  # safe starting point\n",
    "        use_gpu=True,\n",
    "        max_train_samples=2_000,\n",
    "        max_eval_samples=1_000,\n",
    "        shuffle_before_select=True,\n",
    "        seed=seed,\n",
    "        # Optional schema overrides (usually not needed for GLUE)\n",
    "        # text_field=None,\n",
    "        # text_pair_field=None,\n",
    "        # label_field=None,\n",
    "        # task_type=\"auto\",\n",
    "    ),\n",
    "    evaluation_config=BertEvalRequest(\n",
    "        # run_id filled in by coordinator\n",
    "        run_id=None,\n",
    "        dataset_name=\"glue\",\n",
    "        dataset_config_name=\"sst2\",\n",
    "        split=\"validation\",\n",
    "        max_eval_samples=1_000,\n",
    "        max_seq_length=64,\n",
    "        batch_size=32,\n",
    "        use_gpu=True,\n",
    "        seed=seed,\n",
    "        # If your eval requires an explicit model path/URI,\n",
    "        # leave it unset here and let the coordinator fill it.\n",
    "        # model_path=None,\n",
    "        # model_uri=None,\n",
    "    ),\n",
    "    dataset_snapshot=None,  # pass a snapshot if you want strict reproducibility\n",
    ")\n",
    "# Sample 6: {Model: DeBERTa-v3-small - Dataset: Glue}\n",
    "seed = random.randint(0, 10000)\n",
    "config_6 = CoordinatorWorkflowConfig(\n",
    "    fine_tune_config=BertFineTuneConfig(\n",
    "        model_name=\"microsoft/deberta-v3-small\",\n",
    "        dataset_name=\"glue\",\n",
    "        dataset_config_name=\"sst2\",\n",
    "        num_epochs=2,\n",
    "        batch_size=2,  # DeBERTa tends to be heavier on memory (safer on MPS)\n",
    "        learning_rate=2e-5,  # common stable starting LR for DeBERTa fine-tuning\n",
    "        max_seq_length=64,  # start safe; bump to 96/128 once stable\n",
    "        use_gpu=True,\n",
    "        max_train_samples=2_000,\n",
    "        max_eval_samples=1_000,\n",
    "        shuffle_before_select=True,\n",
    "        seed=seed,\n",
    "        # Optional schema overrides (usually not needed for GLUE)\n",
    "        # text_field=None,\n",
    "        # text_pair_field=None,\n",
    "        # label_field=None,\n",
    "        # task_type=\"auto\",\n",
    "    ),\n",
    "    evaluation_config=BertEvalRequest(\n",
    "        run_id=None,  # coordinator fills this in\n",
    "        dataset_name=\"glue\",\n",
    "        dataset_config_name=\"sst2\",\n",
    "        split=\"validation\",\n",
    "        max_eval_samples=1_000,\n",
    "        max_seq_length=64,\n",
    "        batch_size=32,\n",
    "        use_gpu=True,\n",
    "        seed=seed,\n",
    "        # model_path/model_uri left for coordinator to populate from training result\n",
    "    ),\n",
    "    dataset_snapshot=None,\n",
    ")\n",
    "# Sample 7: {Model: SciBERT - Dataset: SciCite}\n",
    "config_7 = CoordinatorWorkflowConfig(\n",
    "    fine_tune_config=BertFineTuneConfig(\n",
    "        model_name=\"allenai/scibert_scivocab_uncased\",\n",
    "        dataset_name=\"scicite\",  # start with SST-2 to validate pipeline\n",
    "        dataset_config_name=\"default\",\n",
    "        use_gpu=True,\n",
    "        max_train_samples=2_000,\n",
    "        max_eval_samples=1_000,\n",
    "        shuffle_before_select=True,\n",
    "    ),\n",
    "    evaluation_config=BertEvalRequest(\n",
    "        run_id=None,  # coordinator fills this in\n",
    "        dataset_name=\"scicite\",\n",
    "        dataset_config_name=\"default\",\n",
    "        split=\"validation\",\n",
    "        max_eval_samples=1_000,\n",
    "        use_gpu=True,\n",
    "    ),\n",
    "    dataset_snapshot=None,  # add snapshot later for reproducibility\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Ladder Sample Configurations\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# Give this sweep a unique experiment identifier so that all per-stage run IDs\n",
    "# can be grouped together in logs and under ``./bert_runs``.\n",
    "ladder_id = uuid.uuid4()  # Replace with custom naming logic as desired.\n",
    "\n",
    "# Sample 1: SciBERT Scaling Ladder\n",
    "ladder_config_1 = SweepRequest(\n",
    "    experiment_id=f\"Bert-ladder-sweep-{ladder_id}\",\n",
    "    base=config_6,\n",
    "    space=SweepSpace(\n",
    "        learning_rate=(5e-5, 1e-5),\n",
    "        batch_size=[\n",
    "            2,\n",
    "            32,\n",
    "        ],  # TODO: Increase batch size in each rung of the ladder, instead of using the ladder to select batch size. Currently, the approach heavily biases towards small batch sizes.\n",
    "        num_epochs=[2, 8],\n",
    "        max_seq_length=[64, 256],\n",
    "    ),\n",
    "    num_trials=12,  # increase this for actual research, but ensure the machine has enough compute and memory or move to an autoscaling cluster\n",
    "    max_concurrency=4,\n",
    "    seed=random.randint(0, 10000),\n",
    ")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Starter Main Function\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "# 1. Connect to Temporal Server using the Pydantic data converter so our\n",
    "# request/response models can be passed directly as workflow arguments.\n",
    "client = await Client.connect(\"localhost:7233\", data_converter=pydantic_data_converter)\n",
    "\n",
    "# 2. Pick the request to run. For the tutorial this is a single ladder\n",
    "# sweep, but you can easily swap in a different ``SweepRequest`` here.\n",
    "request = ladder_config_1\n",
    "\n",
    "# 3. Start the workflow and wait for the result. We call the ``run`` method\n",
    "# on the workflow class directly; Temporal will assign a fresh workflow\n",
    "# execution to the ID provided below.\n",
    "result = await client.execute_workflow(\n",
    "        LadderSweepWorkflow.run,\n",
    "        request,\n",
    "        id=f\"bert-ladder-{ladder_id}\",\n",
    "        task_queue=\"bert-eval-task-queue\",\n",
    "    )\n",
    "\n",
    "# 4. Print a concise, tabular summary of the comparison runs.\n",
    "results = result if isinstance(result, (list, tuple)) else [result]\n",
    "\n",
    "print(\"\\n=== BERT evaluation summary ===\")\n",
    "header = f\"{'run_id':<36} {'dataset':<20} {'split':<10} {'examples':>10} {'accuracy':>9}\"\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "\n",
    "for item in results:\n",
    "    dataset = f\"{item.dataset_name}/{item.dataset_config_name}\"\n",
    "    print(\n",
    "            f\"{item.run_id:<36} \"\n",
    "            f\"{dataset:<20} \"\n",
    "            f\"{item.split:<10} \"\n",
    "            f\"{item.num_examples:>10} \"\n",
    "            f\"{item.accuracy:>9.3f}\",\n",
    "    )\n",
    "\n",
    "# If the ladder workflow annotated the best result with comparison metadata,\n",
    "# print the improvement in accuracy over the baseline run.\n",
    "best = results[0]\n",
    "baseline = getattr(best, \"baseline_accuracy\", None)\n",
    "improvement = getattr(best, \"improvement_vs_baseline\", None)\n",
    "if baseline is not None and improvement is not None:\n",
    "    print(\n",
    "            \"\\nBest run \"\n",
    "            f\"{best.run_id} improved accuracy by {improvement:.3f} \"\n",
    "            f\"over the baseline comparison run \"\n",
    "            f\"(baseline={baseline:.3f}, best={best.accuracy:.3f}).\",\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b5fd26-2ca1-463d-bf6c-972376a2f79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Over the course of our ladder, we tried 21 different hyperparameter configurations.\n",
    "\n",
    "To test 21 options with Grid Search, running each to 8 epochs would require training 168 epochs. In this particular case, \n",
    "we identified the best candidate in 8 epochs, and then spent an additional 50 epochs of budget validating that finding at increasing scale.\n",
    "This is a 95.2% reduction in cost to identify the best configuration, and a 65.4% reduction in overall cost to identify and validate\n",
    "the best configuration compared to using a full grid search. We also have better than random candidates in our later rungs during \n",
    "validation, increasing the ROI on validation spend. \n",
    "\n",
    "In addition, when we compared spending 50% of our budget on the ladder, and then another 50% on training vs spending 100% of budget \n",
    "on a random hyperparam configuration, our model correctly idetnified the sentiment of the sentence 11.5% more often than random even \n",
    "though it saw twice as much training data. (92% accuracy vs 80.5% accuracy)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69d38fe-af81-404c-a328-ed81a417fa0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c579e993-eb27-4dd8-83d0-46da87837de4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Training_Demo)",
   "language": "python",
   "name": "training_demo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
