{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d99bff1-92cd-4285-b475-4b31851ac0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-world checkpoint management \n",
    "\n",
    "def train_with_checkpoints(config):\n",
    "\n",
    "    # Load from checkpoint if exists\n",
    "    if os.path.exists(f\"checkpoint_{config.id}.pt\"):\n",
    "        checkpoint = torch.load(f\"checkpoint_{config.id}.pt\")\n",
    "        model.load_state_dict(checkpoint['model_state'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "\n",
    "    # Otherwise start from scratch\n",
    "    else:\n",
    "        model = create_model()\n",
    "        start_epoch = 0\n",
    "\n",
    "    #Run training\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        train_epoch(model, optimizer)\n",
    "\n",
    "        # Save checkpoint every epoch\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state': model.state_dict(),\n",
    "            'optimizer_state': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "        }, f'checkpoint_{config.id}\n",
    "\n",
    "#---------------------------------------------------------------\n",
    "# How do we orchestrate retries?\n",
    "\n",
    "# What happens if loading the checkpoint fails due to a networking error, but it exists?\n",
    "\n",
    "# Can I locate the path/location of the latest checkpoint in real time?\n",
    "\n",
    "# What if the training dataset changed between runs?\n",
    "#---------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1cbef8f-a77d-4c7f-8b24-a54b740f16dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      8\u001b[39m configs = [\n\u001b[32m      9\u001b[39m     ...\n\u001b[32m     10\u001b[39m ]\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m config \u001b[38;5;129;01min\u001b[39;00m configs:\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     \u001b[43mtrain_model\u001b[49m(config)\n",
      "\u001b[31mNameError\u001b[39m: name 'train_model' is not defined"
     ]
    }
   ],
   "source": [
    "#---------------------------------------------------------------\n",
    "# How do we orchestrate retries?\n",
    "#---------------------------------------------------------------\n",
    "def generate_configs(param_ranges, previous_winners = None, rung = 0) -> [configs]:\n",
    "    ...\n",
    "    return configs\n",
    "\n",
    "def process_results([results: Results]):\n",
    "    ...\n",
    "    return previous_winners\n",
    "\n",
    "\n",
    "x = 5 # Set the number of rungs in the ladder\n",
    "param_ranges ={ \n",
    "    ... # Set param ranges for hyperparameters\n",
    "}\n",
    "\n",
    "n = 0 # Set to initial rung \n",
    "\n",
    "while n < (x-1):\n",
    "    \n",
    "    #1. Generate configurations\n",
    "    configs = generate_configs(param_ranges, previous_winners = previous_winners | None, rung = n)\n",
    "\n",
    "    #2. Initialize results\n",
    "    results = []\n",
    "    \n",
    "    #3. Run training on each configuration \n",
    "    for config in configs:\n",
    "        result = train_model(config)\n",
    "        results.append(result)\n",
    "\n",
    "    #4. Increment the ladder\n",
    "    n += 1\n",
    "\n",
    "#---------------------------------------------------------------\n",
    "# How do we orchestrate retries within the ladder? \n",
    "\n",
    "# What happens if loading a checkpoint fails due to a networking error, but it exists?\n",
    "\n",
    "# Can I locate the path/location of the latest checkpoint in real time?\n",
    "\n",
    "# What if the training dataset changed between runs?\n",
    "#---------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30129637-0f93-44f9-b06c-6a568c01f7c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6b2455-687d-4c4b-9885-d0987e9524cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d997488-397f-4376-b418-a4179e2e90c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import random\n",
    "import uuid\n",
    "\n",
    "from temporalio.client import Client\n",
    "from temporalio.contrib.pydantic import pydantic_data_converter\n",
    "\n",
    "from src.workflows.train_tune.bert_sweeps.custom_types import (\n",
    "    BertEvalRequest,\n",
    "    BertFineTuneConfig,\n",
    "    CoordinatorWorkflowConfig,\n",
    "    SweepRequest,\n",
    "    SweepSpace,\n",
    ")\n",
    "from src.workflows.train_tune.bert_sweeps.workflows import (\n",
    "    LadderSweepWorkflow,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43770a13-1e52-47be-b8c9-b075447a7ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------\n",
    "# Sample Sweep Configurations\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "# Sample 1: {Model: BERT Uncased - Dataset: Glue}\n",
    "config_1 = CoordinatorWorkflowConfig(\n",
    "    fine_tune_config=BertFineTuneConfig(\n",
    "        model_name=\"bert-base-uncased\",\n",
    "        dataset_name=\"glue\",\n",
    "        dataset_config_name=\"sst2\",\n",
    "        num_epochs=2,\n",
    "        batch_size=8,\n",
    "        learning_rate=2e-5,\n",
    "        max_seq_length=128,\n",
    "        use_gpu=True,\n",
    "        max_train_samples=3_000,\n",
    "        max_eval_samples=2_000,\n",
    "        seed=random.randint(0, 10000),\n",
    "    ),\n",
    "    evaluation_config=BertEvalRequest(\n",
    "        dataset_name=\"glue\",\n",
    "        dataset_config_name=\"sst2\",\n",
    "        split=\"validation\",\n",
    "        max_eval_samples=1_000,\n",
    "        max_seq_length=128,\n",
    "        batch_size=32,\n",
    "        use_gpu=True,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Sample 2 {Model: BERT Cased - Dataset: Glue}\n",
    "config_2 = CoordinatorWorkflowConfig(\n",
    "    fine_tune_config=BertFineTuneConfig(\n",
    "        model_name=\"bert-base-cased\",\n",
    "        dataset_name=\"glue\",\n",
    "        dataset_config_name=\"sst2\",\n",
    "        num_epochs=10,\n",
    "        batch_size=16,\n",
    "        learning_rate=3e-5,\n",
    "        max_seq_length=128,\n",
    "        use_gpu=True,\n",
    "        max_train_samples=3_000,\n",
    "        max_eval_samples=2_000,\n",
    "        seed=random.randint(0, 10000),\n",
    "    ),\n",
    "    evaluation_config=BertEvalRequest(\n",
    "        dataset_name=\"glue\",\n",
    "        dataset_config_name=\"sst2\",\n",
    "        split=\"validation\",\n",
    "        max_eval_samples=1_000,\n",
    "        max_seq_length=128,\n",
    "        batch_size=32,\n",
    "        use_gpu=True,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Sample 3 {Model: BERT Uncased - Dataset: IMDB}\n",
    "seed = random.randint(0, 10000)\n",
    "config_3 = CoordinatorWorkflowConfig(\n",
    "    fine_tune_config=BertFineTuneConfig(\n",
    "        model_name=\"bert-base-uncased\",\n",
    "        dataset_name=\"imdb\",\n",
    "        dataset_config_name=\"plain_text\",\n",
    "        num_epochs=10,\n",
    "        batch_size=32,\n",
    "        learning_rate=2e-5,\n",
    "        max_seq_length=256,\n",
    "        use_gpu=True,\n",
    "        max_train_samples=5_000,\n",
    "        max_eval_samples=2_000,\n",
    "        seed=seed,\n",
    "    ),\n",
    "    evaluation_config=BertEvalRequest(\n",
    "        dataset_name=\"imdb\",\n",
    "        dataset_config_name=\"plain_text\",\n",
    "        split=\"test\",\n",
    "        max_eval_samples=1_000,\n",
    "        max_seq_length=256,\n",
    "        batch_size=32,\n",
    "        use_gpu=True,\n",
    "        seed=seed,\n",
    "    ),\n",
    ")\n",
    "# Sample 4: {Model: DistilBERT - Dataset: Glue}\n",
    "seed = random.randint(0, 10000)\n",
    "config_4 = CoordinatorWorkflowConfig(\n",
    "    fine_tune_config=BertFineTuneConfig(\n",
    "        model_name=\"distilbert-base-uncased\",\n",
    "        dataset_name=\"glue\",\n",
    "        dataset_config_name=\"sst2\",\n",
    "        num_epochs=2,\n",
    "        batch_size=4,  # MPS-safe\n",
    "        learning_rate=5e-5,\n",
    "        max_seq_length=64,  # MPS-safe\n",
    "        use_gpu=True,\n",
    "        max_train_samples=2_000,\n",
    "        max_eval_samples=1_000,\n",
    "        shuffle_before_select=True,\n",
    "        seed=seed,\n",
    "        # optional overrides if you added them:\n",
    "        # text_field=None,\n",
    "        # text_pair_field=None,\n",
    "        # label_field=None,\n",
    "        # task_type=\"auto\",\n",
    "    ),\n",
    "    evaluation_config=BertEvalRequest(\n",
    "        # run_id will be filled in by the coordinator after training\n",
    "        run_id=None,\n",
    "        dataset_name=\"glue\",\n",
    "        dataset_config_name=\"sst2\",\n",
    "        split=\"validation\",\n",
    "        max_eval_samples=1_000,\n",
    "        max_seq_length=64,\n",
    "        batch_size=32,\n",
    "        use_gpu=True,\n",
    "        seed=seed,\n",
    "        # if you changed eval to require model_uri/model_path, leave it unset here\n",
    "        # and let the coordinator populate it from the training result.\n",
    "        # model_path=None,\n",
    "        # model_uri=None,\n",
    "    ),\n",
    "    dataset_snapshot=None,  # or pass a DatasetSnapshotResult if you want reproducibility\n",
    ")\n",
    "\n",
    "# Sample 5: {Model: MiniLM-L12-H384-uncased - Dataset: Glue}\n",
    "seed = random.randint(0, 10000)\n",
    "config_5 = CoordinatorWorkflowConfig(\n",
    "    fine_tune_config=BertFineTuneConfig(\n",
    "        model_name=\"microsoft/MiniLM-L12-H384-uncased\",\n",
    "        dataset_name=\"glue\",\n",
    "        dataset_config_name=\"sst2\",\n",
    "        num_epochs=2,\n",
    "        batch_size=4,  # still MPS-safe\n",
    "        learning_rate=3e-5,  # MiniLM often prefers slightly lower LR\n",
    "        max_seq_length=64,  # safe starting point\n",
    "        use_gpu=True,\n",
    "        max_train_samples=2_000,\n",
    "        max_eval_samples=1_000,\n",
    "        shuffle_before_select=True,\n",
    "        seed=seed,\n",
    "        # Optional schema overrides (usually not needed for GLUE)\n",
    "        # text_field=None,\n",
    "        # text_pair_field=None,\n",
    "        # label_field=None,\n",
    "        # task_type=\"auto\",\n",
    "    ),\n",
    "    evaluation_config=BertEvalRequest(\n",
    "        # run_id filled in by coordinator\n",
    "        run_id=None,\n",
    "        dataset_name=\"glue\",\n",
    "        dataset_config_name=\"sst2\",\n",
    "        split=\"validation\",\n",
    "        max_eval_samples=1_000,\n",
    "        max_seq_length=64,\n",
    "        batch_size=32,\n",
    "        use_gpu=True,\n",
    "        seed=seed,\n",
    "        # If your eval requires an explicit model path/URI,\n",
    "        # leave it unset here and let the coordinator fill it.\n",
    "        # model_path=None,\n",
    "        # model_uri=None,\n",
    "    ),\n",
    "    dataset_snapshot=None,  # pass a snapshot if you want strict reproducibility\n",
    ")\n",
    "# Sample 6: {Model: DeBERTa-v3-small - Dataset: Glue}\n",
    "seed = random.randint(0, 10000)\n",
    "config_6 = CoordinatorWorkflowConfig(\n",
    "    fine_tune_config=BertFineTuneConfig(\n",
    "        model_name=\"microsoft/deberta-v3-small\",\n",
    "        dataset_name=\"glue\",\n",
    "        dataset_config_name=\"sst2\",\n",
    "        num_epochs=2,\n",
    "        batch_size=2,  # DeBERTa tends to be heavier on memory (safer on MPS)\n",
    "        learning_rate=2e-5,  # common stable starting LR for DeBERTa fine-tuning\n",
    "        max_seq_length=64,  # start safe; bump to 96/128 once stable\n",
    "        use_gpu=True,\n",
    "        max_train_samples=2_000,\n",
    "        max_eval_samples=1_000,\n",
    "        shuffle_before_select=True,\n",
    "        seed=seed,\n",
    "        # Optional schema overrides (usually not needed for GLUE)\n",
    "        # text_field=None,\n",
    "        # text_pair_field=None,\n",
    "        # label_field=None,\n",
    "        # task_type=\"auto\",\n",
    "    ),\n",
    "    evaluation_config=BertEvalRequest(\n",
    "        run_id=None,  # coordinator fills this in\n",
    "        dataset_name=\"glue\",\n",
    "        dataset_config_name=\"sst2\",\n",
    "        split=\"validation\",\n",
    "        max_eval_samples=1_000,\n",
    "        max_seq_length=64,\n",
    "        batch_size=32,\n",
    "        use_gpu=True,\n",
    "        seed=seed,\n",
    "        # model_path/model_uri left for coordinator to populate from training result\n",
    "    ),\n",
    "    dataset_snapshot=None,\n",
    ")\n",
    "# Sample 7: {Model: SciBERT - Dataset: SciCite}\n",
    "config_7 = CoordinatorWorkflowConfig(\n",
    "    fine_tune_config=BertFineTuneConfig(\n",
    "        model_name=\"allenai/scibert_scivocab_uncased\",\n",
    "        dataset_name=\"scicite\",  # start with SST-2 to validate pipeline\n",
    "        dataset_config_name=\"default\",\n",
    "        use_gpu=True,\n",
    "        max_train_samples=2_000,\n",
    "        max_eval_samples=1_000,\n",
    "        shuffle_before_select=True,\n",
    "    ),\n",
    "    evaluation_config=BertEvalRequest(\n",
    "        run_id=None,  # coordinator fills this in\n",
    "        dataset_name=\"scicite\",\n",
    "        dataset_config_name=\"default\",\n",
    "        split=\"validation\",\n",
    "        max_eval_samples=1_000,\n",
    "        use_gpu=True,\n",
    "    ),\n",
    "    dataset_snapshot=None,  # add snapshot later for reproducibility\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bb34f21-ddac-40a4-b0e2-ff2ee72f0309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Ladder Sample Configurations\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# Give this sweep a unique experiment identifier so that all per-stage run IDs\n",
    "# can be grouped together in logs and under ``./bert_runs``.\n",
    "ladder_id = uuid.uuid4()  # Replace with custom naming logic as desired.\n",
    "\n",
    "# Sample 1: SciBERT Scaling Ladder\n",
    "ladder_config_1 = SweepRequest(\n",
    "    experiment_id=f\"Bert-ladder-sweep-{ladder_id}\",\n",
    "    base=config_6,\n",
    "    space=SweepSpace(\n",
    "        learning_rate=(5e-5, 1e-5),\n",
    "        batch_size=[\n",
    "            2,\n",
    "            32,\n",
    "        ],  # TODO: Increase batch size in each rung of the ladder, instead of using the ladder to select batch size. Currently, the approach heavily biases towards small batch sizes.\n",
    "        num_epochs=[2, 8],\n",
    "        max_seq_length=[64, 256],\n",
    "    ),\n",
    "    num_trials=12,  # increase this for actual research, but ensure the machine has enough compute and memory or move to an autoscaling cluster\n",
    "    max_concurrency=4,\n",
    "    seed=random.randint(0, 10000),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45d78951-1baa-4a1a-acc9-440a44d0791e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# Starter Function\n",
    "# ------------------------------------------------------------------------------\n",
    "async def main() -> None:\n",
    "    # 1. Connect to Temporal Server using the Pydantic data converter so our\n",
    "    # request/response models can be passed directly as workflow arguments.\n",
    "    client = await Client.connect(\"localhost:7233\", data_converter=pydantic_data_converter)\n",
    "\n",
    "    # 2. Pick the request to run. For the tutorial this is a single ladder\n",
    "    # sweep, but you can easily swap in a different ``SweepRequest`` here.\n",
    "    request = ladder_config_1\n",
    "\n",
    "    # 3. Start the workflow and wait for the result. We call the ``run`` method\n",
    "    # on the workflow class directly; Temporal will assign a fresh workflow\n",
    "    # execution to the ID provided below.\n",
    "    result = await client.execute_workflow(\n",
    "        LadderSweepWorkflow.run,\n",
    "        request,\n",
    "        id=f\"bert-ladder-{ladder_id}\",\n",
    "        task_queue=\"bert-eval-task-queue\",\n",
    "    )\n",
    "\n",
    "    # 4. Print a concise, tabular summary of the winning candidate & result.\n",
    "    results = result if isinstance(result, (list, tuple)) else [result]\n",
    "\n",
    "    print(\"\\n=== BERT evaluation summary ===\")\n",
    "    header = f\"{'run_id':<36} {'dataset':<20} {'split':<10} {'examples':>10} {'accuracy':>9}\"\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "\n",
    "    for item in results:\n",
    "        dataset = f\"{item.dataset_name}/{item.dataset_config_name}\"\n",
    "        print(\n",
    "            f\"{item.run_id:<36} \"\n",
    "            f\"{dataset:<20} \"\n",
    "            f\"{item.split:<10} \"\n",
    "            f\"{item.num_examples:>10} \"\n",
    "            f\"{item.accuracy:>9.3f}\",\n",
    "        )\n",
    "\n",
    "    # If the ladder workflow annotated the best result with ablation metadata,\n",
    "    # print the improvement in accuracy over the ablation baseline.\n",
    "    best = results[0]\n",
    "    baseline = getattr(best, \"baseline_accuracy\", None)\n",
    "    improvement = getattr(best, \"improvement_vs_baseline\", None)\n",
    "    if baseline is not None and improvement is not None:\n",
    "        print(\n",
    "            \"\\nBest run \"\n",
    "            f\"{best.run_id} improved accuracy by {improvement:.3f} \"\n",
    "            f\"over the ablation baseline \"\n",
    "            f\"(baseline={baseline:.3f}, best={best.accuracy:.3f}).\",\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "415c27d2-214d-441d-b963-531a0ca6fcc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== BERT evaluation summary ===\n",
      "run_id                               dataset              split        examples  accuracy\n",
      "-----------------------------------------------------------------------------------------\n",
      "Bert-ladder-sweep-945358ec-230c-418e-b2fe-553c11f8e242-s2-0002 glue/sst2            validation        872     0.920\n",
      "\n",
      "Best run Bert-ladder-sweep-945358ec-230c-418e-b2fe-553c11f8e242-s2-0002 improved accuracy by 0.091 over the ablation baseline (baseline=0.829, best=0.920).\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# Starter\n",
    "# ------------------------------------------------------------------------------\n",
    "result = await main()\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d89f29-b7ad-48fe-a91a-ab500dbabcc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Training_Demo)",
   "language": "python",
   "name": "training_demo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
